{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaner Notebook\n",
    "Appends pipeline reports together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import dateutil\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define Functions\n",
    "\n",
    "def clean_df(df):\n",
    "    # Standardize best_stat\n",
    "    df.best_stat = df.best_stat.apply(lambda x: x.strip().upper())\n",
    "\n",
    "    # Standardize best_date\n",
    "    df.best_date = df.best_date.apply(maybe_format_date)\n",
    "    \n",
    "    # Standardize apn to 7 digits (with leading 0)\n",
    "    df['apn']=df['apn'].apply(lambda x: x.zfill(7))\n",
    "    \n",
    "    # Compile \"First Filed\" variable\n",
    "    df['firstfiled'] = pd.to_datetime(df['firstfiled'])\n",
    "    df['planning_filed'] = pd.to_datetime(df['planning_filed'])\n",
    "    df['dbi_filed'] = pd.to_datetime(df['dbi_filed'])\n",
    "    \n",
    "    def rule(value):\n",
    "        if pd.isnull(value['firstfiled']):\n",
    "            if pd.isnull(value['planning_filed']) & pd.notnull(value['dbi_filed']):\n",
    "                return value['dbi_filed']\n",
    "            elif pd.isnull(value['dbi_filed']) & pd.notnull(value['planning_filed']):\n",
    "                return value['planning_filed']\n",
    "            elif pd.notnull(value['dbi_filed']) & pd.notnull(value['planning_filed']):\n",
    "                return value[['planning_filed', 'dbi_filed']].min()\n",
    "        else:\n",
    "            return value['firstfiled']\n",
    "    \n",
    "    df['firstfiled'] = df.apply(rule, axis = 1)\n",
    "    \n",
    "\n",
    "    # Standardize APN\n",
    "    # Some APN values are \"APN XXXXXX\", some are just \"XXXXXXX\", standardize this.\n",
    "    records_with_apn = df.apn.notnull()\n",
    "    df.apn = df.apn[records_with_apn].apply(lambda x: x.split()[-1])\n",
    "\n",
    "    # Standardize Lat/Long\n",
    "    records_with_location_attribute = df.location.notnull()\n",
    "    if 'x' not in df.columns:\n",
    "        df.x = df.location[records_with_location_attribute].apply(get_lat_from_glob)\n",
    "        df.y = df.location[records_with_location_attribute].apply(get_long_from_glob)\n",
    "    elif 'x' in df.columns:\n",
    "        df.x[records_with_location_attribute] = df.location[records_with_location_attribute].apply(get_lat_from_glob)\n",
    "        df.y[records_with_location_attribute] = df.location[records_with_location_attribute].apply(get_long_from_glob)\n",
    "\n",
    "    # Get address into separate fields for cases where it is concatenated with the lat,long\n",
    "    if 'address' not in df.columns:\n",
    "        records_with_address_in_location_attribute = df.location.apply(lambda x: not pd.isnull(x) and '\\n' in x)\n",
    "        df.address = df.location[records_with_address_in_location_attribute].apply(get_address_from_glob)\n",
    "    else:\n",
    "        df['address2'] = df.address #df.address2 syntax would result in chained assignment\n",
    "        records_with_address_in_location_attribute = df.location.apply(lambda x: not pd.isnull(x) and '\\n' in x) #and df.address.apply(lambda x: pandas.isnull(x)) \n",
    "        df.address[records_with_address_in_location_attribute] = df.location[records_with_address_in_location_attribute].apply(get_address_from_glob)\n",
    "        records_with_address_field = df.address2.notnull() #need additional layer of cleaning because some filed have zip codes in location field instead of lat long. These usually have separate address field to use. \n",
    "        df.address[records_with_address_field] = df.address2[records_with_address_field]\n",
    "\n",
    "    # Standardize address by uppercasing and removing punctuation\n",
    "    df.address = df.address.apply(standardize_address)\n",
    "\n",
    "    # Classify neighborhoods using point-in-polygon approach. Brian: Skipping this for now\n",
    "    #df['classified_neighborhood'] = geo_classifier.classify_df(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def maybe_format_date(s):\n",
    "    \"\"\"\n",
    "    Use advanced date parsing (python-dateutil) to parse\n",
    "    the first 10 chars of each line.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return dateutil.parser.parse(s.strip()[:10]).strftime(\"%Y-%m-%d\")\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Date formatting failed for {}\".format(s))\n",
    "        return s\n",
    "    \n",
    "def get_coords_tuple_from_address_lat_long_glob(s):\n",
    "    \"\"\"\n",
    "    For some quarters, the address and lat/long tuple are\n",
    "    in the same column, concatenated together with a newline\n",
    "\n",
    "    Returns: tuple\n",
    "    \"\"\"\n",
    "    lat_long_tuple = s.split('\\n')[-1]\n",
    "    # Dirty hack, the lat long tuple happens to be valid python syntax so.. YOLO\n",
    "    return eval(lat_long_tuple)\n",
    "\n",
    "\n",
    "def get_lat_from_glob(s):\n",
    "    try:\n",
    "        return get_coords_tuple_from_address_lat_long_glob(s)[0]\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Lat long glob parsing failed for {}\".format(s))\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_long_from_glob(s):\n",
    "    try:\n",
    "        return get_coords_tuple_from_address_lat_long_glob(s)[1]\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Lat long glob parsing failed for {}\".format(s))\n",
    "        return np.nan\n",
    "    \n",
    "def get_address_from_glob(s):\n",
    "    return s.split('\\n')[0]\n",
    "\n",
    "def standardize_address(s):\n",
    "    return s.upper().replace('.', '').replace(',', '')\n",
    "\n",
    "def main():\n",
    "    ################################################################################\n",
    "    # Allows for a map of column names to be used to convert field names and output\n",
    "    # the result into a dict object\n",
    "    #\n",
    "    def load_csv_with_mapping(csvfile, column_mapping):\n",
    "        mapped_data = []\n",
    "        for row in csv.DictReader(csvfile):\n",
    "            mapped_row = dict((column_mapping[k], v) for k, v in row.items())\n",
    "            mapped_data.append(mapped_row)\n",
    "        return mapped_data\n",
    "    \n",
    "     ################################################################################\n",
    "    # Given a file with the column names, create a list of dictionaries with\n",
    "    # additional entries for the year and quarter created from the date field to\n",
    "    # ease future transforms\n",
    "    #\n",
    "    # see https://docs.google.com/spreadsheets/d/1ikjaHDLf-iCGBCQ1KmSIXVEiVNbX8pQzW26yYqhrH3U/edit#gid=1633784412\n",
    "    #\n",
    "    column_mappings = []\n",
    "    for mapping_filename in os.listdir('raw/columnnames/'):\n",
    "        with open('raw/columnnames/' + mapping_filename, 'r') as mapping_file:\n",
    "            column_mapping = {}\n",
    "            column_mapping['year'] = mapping_filename[0:4]\n",
    "            column_mapping['quarter'] = mapping_filename[5:6]\n",
    "            for row in csv.DictReader(mapping_file):\n",
    "                column_mapping[row['key']] = row['value']\n",
    "            column_mappings.append(column_mapping)\n",
    "\n",
    "\n",
    "    all_housing_data = pd.DataFrame()\n",
    "    \n",
    "    ################################################################################\n",
    "    # Load each file based on filename conventions and apply the column cleaning\n",
    "    #\n",
    "    for column_mapping in column_mappings:\n",
    "        filename = 'San_Francisco_Development_Pipeline_%s_Quarter_%s' % (\n",
    "            column_mapping['year'],\n",
    "            column_mapping['quarter'],\n",
    "        )\n",
    "\n",
    "        housing_data_csv_filename = 'raw/' + filename + '.csv'\n",
    "        print('loading ' + housing_data_csv_filename)\n",
    "\n",
    "\n",
    "        with open(housing_data_csv_filename, 'r') as csvfile:\n",
    "            housing_data = load_csv_with_mapping(csvfile, column_mapping)\n",
    "            housing_data_json_filename = 'cleaned/' + filename + '.json'\n",
    "            df = pd.DataFrame(housing_data)\n",
    "            df['original_data_file'] = housing_data_csv_filename\n",
    "            df['report_year'] = housing_data_csv_filename.replace(\".csv\",'').split(\"_\")[4]\n",
    "            df['report_quarter'] = housing_data_csv_filename.replace(\".csv\",'').split(\"_\")[6]\n",
    "            all_housing_data = all_housing_data.append(df, ignore_index=True)\n",
    "            print('writing ' + housing_data_json_filename)\n",
    "            df.to_json(housing_data_json_filename, orient='records')\n",
    "    \n",
    "    all_housing_data.to_csv(\"cleaned/all_quarters_merged_PRECLEAN.csv\")\n",
    "    df = clean_df(all_housing_data)\n",
    "    df.to_csv(\"cleaned/all_quarters_merged.csv\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading raw/San_Francisco_Development_Pipeline_2011_Quarter_1.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2011_Quarter_1.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2011_Quarter_2.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2011_Quarter_2.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2011_Quarter_3.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2011_Quarter_3.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2011_Quarter_4.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2011_Quarter_4.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2012_Quarter_1.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2012_Quarter_1.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2012_Quarter_2.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2012_Quarter_2.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2012_Quarter_3.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2012_Quarter_3.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2012_Quarter_4.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2012_Quarter_4.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2013_Quarter_1.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2013_Quarter_1.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2013_Quarter_2.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2013_Quarter_2.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2013_Quarter_3.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2013_Quarter_3.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2013_Quarter_4.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2013_Quarter_4.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2014_Quarter_1.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2014_Quarter_1.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2014_Quarter_2.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2014_Quarter_2.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2014_Quarter_3.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2014_Quarter_3.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2014_Quarter_4.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2014_Quarter_4.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2015_Quarter_1.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2015_Quarter_1.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2015_Quarter_2.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2015_Quarter_2.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2015_Quarter_3.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2015_Quarter_3.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2015_Quarter_4.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2015_Quarter_4.json\n",
      "loading raw/San_Francisco_Development_Pipeline_2016_Quarter_1.csv\n",
      "writing cleaned/San_Francisco_Development_Pipeline_2016_Quarter_1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briangoggin/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/briangoggin/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/briangoggin/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/briangoggin/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#run code\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
